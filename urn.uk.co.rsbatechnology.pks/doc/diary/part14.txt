= Part 14: Performance Engineering =

__TOC__

Now I had the core of the PKS application working, and a WWW simulator application that could generate root requests into NetKernel, I was ready to experiment with using NetKernel's unique architectural context layers to control the performance characteristics of the application.

As a recap from [[doc:uk:co:rsbatechnology:pks:diary:part1 | Part 1]], there were two critical performance objectives stated in the requirements:

{| border="1"
|-
! Tag:
| P1.4 Position Capacity
! Type:
| Performance
|-
| Ambition:
| colspan="3" | Maintain future-dated positions generated by expected trading volumes over next 1 year
|-
! Scale:
| colspan="3" | The number of uniquely-identifiable Non-Historical Currency Positions that can be stored and retrieved by the System for a given <nowiki>[Business Area]</nowiki> and <nowiki>[Number of Amount Types]</nowiki>

; Non-Historical: :  Any Currency Position with a Position Date greater than or equal to the current Global Business Date
|-
! Goal:
| colspan="3" | <nowiki>[Business Area=EFX, Amount Types={symbol, base}, NK Prototype End May 2014]</nowiki>  20,000 
|}

and

{| border="1"
|-
! Tag:
| F2.1.3 Transaction Throughput
! Type:
| Performance
|-
! Ambition:
| colspan="3" | Handle peak flow expected number of new transactions whilst not exceeding maximum time for publishing new position changes|
|-
! Scale:
| colspan="3" | <nowiki>The maximum number of new Position-Originating Transactions received and processed during a [Peak Flow Period] whilst not exceeding [Maximum Per-Position Change Publication Time]</nowiki>

; Per-Position Change Publication Time (MPPCPT): :  Time taken from receipt of a Position-Originating Transaction by the System to publication of all corresponding ''Position Updates'' '''(see Position Publication section)'''
|-
! Goal:
| colspan="3" | <nowiki>[Peak Flow Period=10 seconds, MPPCPT=10 ms, NK Prototype End May 2014]</nowiki>  100 
|}


== Performance Simulation ==

Before tackling performance architecture, I needed to build a performance simulator.  In a real-world system, we could imagine having hooked up the NetKernel PKS to a wider message-based system (by developing an appropriate [[doc:glossary:transport | Transport space]]) .  The PKS would be load-tested by transmitting external messages and treating the entire PKS instance as a black box.  However, in this experimental version, we will use the WWW simulator to trigger multiple random source transaction requests. 

My first attempt at this used javascript in the WWW '''simulator.html''' to generate periodic random requests.  But I encountered two problems with this:
* I ran the simulator through the NetKernel Admin Portal (http://localhost:1060/pks/pub/simulator).  But for some reason, I could not achieve a throughput of more than a couple of requests per second. I also encountered numerous out-of-memory errors.  It turns out (thanks to Peter for pointing this out), that the admin portal is only configured to handle a small number of requests per second, and so was queuing up the test requests.
* Each root request was going right through the Browser, local network, HTTP Transport and NK Admin portal stack before reaching the PKS code. Although the latency was fairly small, it was also suggested I take another approach (see below).

The first issue was more easily dealt with, by exposing the PKS WWW application to the NetKernel Frontend space (see the '''SimpleDynamicImportHook.xml''' file in the PKS WWW module), which does not have the same performance restrictions as the admin backend.  The simulator could now be reached on the default front-end root URL (as well as the back-end ... well, why not?):

http://localhost:8080/pks/pub/simulator

My second attempt focused on avoiding the HTTP stack, by utilising NetKernel's built-in [[doc:mod:cron:title | CRON]] scheduler.  This allowed me to build a new endpoint in the WWW application that created a CRON job that periodically generated a random source transaction root request.  Because each CRON task inherits the request context of the endpoint which created it, these root requests would resolve to the PKS application endpoints.

First, I needed a CRON job definition xml configuration resource.  I wanted to configure the number of repeats and repeat intervals from the WWW simulator page, so needed a way of embedding arguments into the xml.  I built this as a Freemarker string template:

{xml}
<job>
    <name>PKSSendMultipleTrades</name>
    <desc>Multiple Random Trade Simulator.</desc>
    <request>
      <identifier>res:/pks/sendrandomtrade</identifier>
    </request>
    <simple>
      <startTime />
      <endTime />
      <repeatCount>${repeatCount}</repeatCount>
      <repeatInterval>${repeatInterval}</repeatInterval>
    </simple>
</job>
{/xml}

This configuration create a CRON job that issues a root request to '''res:/pks/sendrandomtrade''', starting immediately the job is created (via the empty <code><startTime /></code> element), and repeating <code>${repeatInterval}</code> times.

Then, the '''sendMultipleTrades''' endpoint, implemented as a DPML script, could call <code>active:freemarker</code> with the template and the arguments passed down from the calling request:

{xml}
<sequence>
	<request assignment="cronJobDefinition">
	    		<identifier>active:freemarker</identifier>
	    		<argument name="repeatCount">arg:repeatCount</argument>
	    		<argument name="repeatInterval">arg:repeatInterval</argument>
	    		<argument name="operator">res:/resources/endpoints/sendMultipleTrades.ftl</argument>
	</request>
	<request assignment="response">
	    <identifier>active:cronNew</identifier>
	    <argument name="job">this:cronJobDefinition</argument>
	</request>
</sequence>
{/xml}

Note that this DPML script uses a variable assignment to '''cronJobDefinition''', with the returned representation of the freemarker call (containing the <code><job></code> configuration document with the repeat arguments filled-in), which is passed into the '''active:cronNew''' CRON service request as the '''job''' argument.

Now the endpoint had been Constructed (without needing to write any physical code), we Compose the logical endpoint in the WWW space that is called from the '''simulator.html''' page:

{xml}
            <endpoint>
               <grammar>
               		<simple>res:/pks/sendmultipletrades/{interval}/{repeats}</simple>
               </grammar>
              <request>
                  <identifier>active:dpml</identifier>
                  <argument name="operator">res:/resources/endpoints/sendMultipleTrades.xml</argument>
                  <argument name="repeatCount" method="as-string">[[arg:repeats]]</argument>
	    		  <argument name="repeatInterval" method="as-string">[[arg:interval]]</argument>
               </request>
            </endpoint>
{/xml}

<blockquote>
It took me a little while to figure out how to get the '''repeats''' and '''interval''' argument values passed down into the CRON <code><job></code> xml resource, hence the perhaps odd use of freemarker string templating instead of XML - related technology (e.g. XRL2).  Something to look at again at some point...  
</blockquote>

Finally, it was now trivial to add some further UI components to the web simulator to trigger a HTTP GET request to resolve to the '''res:/pks/sendmultipletrades/{interval}/{repeats}''' URL.

Take a look at http://localhost:8080/pks/pub/simulator on your own system.

I could now trigger a sequence of periodic random simulated source transaction requests.  The NetKernel Visualiser showed the multiple repeated root requests with the '''res:/pks/sendrandomtrade''' identifier.

== Testing Transaction Throughput ==

=== Test Execution ===
The first performance target I tested was the Transaction Throughput requirement.  The goal was to achieve a sustained performance of approx 10ms per source transaction, with a received volume of 100 requests in a 10 second period (let's say an average of 100ms per request, ignoring finer - grained volume peaks of periods much less than 10 seconds).

Using the enhanced WWW simulator, I set up a run of 100 transactions, spaced out by an interval of 100ms, turned on the Visualiser, and clicked the '''Send multiple random''' button.  After an anxious wait of 10 seconds, I stopped the Visualiser.  

=== Test Outcome ===
The first good news was NK had taken the brunt of the load test, apparently with no ill effect.  The [[http://localhost:1060/tools/statuscharts | Status Graphs]] page showed a peak of approximateky 50 requests / second, and an increase in OldGen Heap usage of approx 50Mb (I suspect this was mostly the Visualiser traces being held in memory).  The CPU utilisation peaked briefly at 20% on my MacBookPro 2009. 

The second interesting thing to note was that the Visualiser table was now reporting it held traces of 104 root requests.      The shortest duration '''res:/pks/sendrandomtrade''' request was reported to be  23ms, the longest at 2485 ms (also the last) the second longest at 175ms.  

I could now inspect the complete Visualiser "time machine" trace information for a sample of these requests to quickly pinpoint where the time was being spent (it sure beats trawling through 10ms of Mbs of log4j application logs to extract end to end timing information!)

The 2 sub-requests of each root request that had the highest reported individual CPU usage (found easily by sorting the Visualiser request trace table by the '''CPU(ms)''' column) were '''active:pds''' SINK requests (5 - 10 ms each), followed by several HDSToXML TRANSREPT requests issued by the '''PDSH2Impl''' endpoint (each between 1.00 and 1.25 ms).  

The outlier request of 2485 ms reported a single XMLToHDS TRANSREPT request for an identifier '''pds:/pks/pos/EFX:LDN:PRE_STP:5039a5d7-0c5c-4ef5-a5f4-0276b55198dc:OPEN:2014-05-19:HKD''' with a CPU(ms) of 2461.87 ms (maybe there was a JVM GC collection at that point?)

=== Test Conclusion ===
<blockquote>
It is clear that our goal of '''10ms''' end-to-end has been exceeded between +100% to +500% in the current implementation.  However, it appears this is almost entirely (aside from a single as-yet unexplained outlier case) related to the NetKernel PDS persistence engine and the underlying H2 embedded database, at least as running on my MacBook (despite it having a [http://www.samsung.com/uk/consumer/memory-cards-hdd-odd/ssd/840-pro SSD drive] !). 
</blockquote>

Although not surprising, it highlights the old adage of avoiding direct SQL calls from a low-latency , high-throughput application if at all possible.  In the original, real-world platform in which the Java OO - based PKS operates,  it used a very fast write-only local persistence event source file (for restoring cache state on re-start) in line with the business logic.  An entirely separate Java application running on a different server was responsible for persisting trade and position message contents published on the middleware message bus into an archival SQL database. 

Also, Peter Rodgers pointed out to me that the PDS / embedded H2 combination is really only intended as a development tool, and certainly would not be recommended for use by NK-PKS in a production environment! 


