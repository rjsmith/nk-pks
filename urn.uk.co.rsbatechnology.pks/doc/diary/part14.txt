= Part 14: Performance Engineering =

__TOC__

Now I had the core of the PKS application working, and a WWW simulator application that could generate root requests into NetKernel, I was ready to experiment with using NetKernel's unique architectural context layers to control the performance characteristics of the application.

As a recap from [[doc:uk:co:rsbatechnology:pks:diary:part1 | Part 1]], there were two critical performance objectives stated in the requirements:

{| border="1"
|-
! Tag:
| P1.4 Position Capacity
! Type:
| Performance
|-
| Ambition:
| colspan="3" | Maintain future-dated positions generated by expected trading volumes over next 1 year
|-
! Scale:
| colspan="3" | The number of uniquely-identifiable Non-Historical Currency Positions that can be stored and retrieved by the System for a given <nowiki>[Business Area]</nowiki> and <nowiki>[Number of Amount Types]</nowiki>

; Non-Historical: :  Any Currency Position with a Position Date greater than or equal to the current Global Business Date
|-
! Goal:
| colspan="3" | <nowiki>[Business Area=EFX, Amount Types={symbol, base}, NK Prototype End May 2014]</nowiki>  20,000 
|}

and

{| border="1"
|-
! Tag:
| F2.1.3 Transaction Throughput
! Type:
| Performance
|-
! Ambition:
| colspan="3" | Handle peak flow expected number of new transactions whilst not exceeding maximum time for publishing new position changes|
|-
! Scale:
| colspan="3" | <nowiki>The maximum number of new Position-Originating Transactions received and processed during a [Peak Flow Period] whilst not exceeding [Maximum Per-Position Change Publication Time]</nowiki>

; Per-Position Change Publication Time (MPPCPT): :  Time taken from receipt of a Position-Originating Transaction by the System to publication of all corresponding ''Position Updates'' '''(see Position Publication section)'''
|-
! Goal:
| colspan="3" | <nowiki>[Peak Flow Period=10 seconds, MPPCPT=10 ms, NK Prototype End May 2014]</nowiki>  100 
|}


== Performance Simulation ==

Before tackling performance architecture, I needed to build a performance simulator.  In a real-world system, we could imagine having hooked up the NetKernel PKS to a wider message-based system (by developing an appropriate [[doc:glossary:transport | Transport space]]) .  The PKS would be load-tested by transmitting external messages and treating the entire PKS instance as a black box.  However, in this experimental version, we will use the WWW simulator to trigger multiple random source transaction requests. 

My first attempt at this used javascript in the WWW '''simulator.html''' to generate periodic random requests.  But I encountered two problems with this:
* I ran the simulator through the NetKernel Admin Portal (http://localhost:1060/pks/pub/simulator).  But for some reason, I could not achieve a throughput of more than a couple of requests per second. I also encountered numerous out-of-memory errors.  It turns out (thanks to Peter for pointing this out), that the admin portal is only configured to handle a small number of requests per second, and so was queuing up the test requests.
* Each root request was going right through the Browser, local network, HTTP Transport and NK Admin portal stack before reaching the PKS code. Although the latency was fairly small, it was also suggested I take another approach (see below).

The first issue was more easily dealt with, by exposing the PKS WWW application to the NetKernel Frontend space (see the '''SimpleDynamicImportHook.xml''' file in the PKS WWW module), which does not have the same performance restrictions as the admin backend.  The simulator could now be reached on the default front-end root URL (as well as the back-end ... well, why not?):

http://localhost:8080/pks/pub/simulator

My second attempt focused on avoiding the HTTP stack, by utilising NetKernel's built-in [[doc:mod:cron:title | CRON]] scheduler.  This allowed me to build a new endpoint in the WWW application that created a CRON job that periodically generated a random source transaction root request.  Because each CRON task inherits the request context of the endpoint which created it, these root requests would resolve to the PKS application endpoints.

First, I needed a CRON job definition xml configuration resource.  I wanted to configure the number of repeats and repeat intervals from the WWW simulator page, so needed a way of embedding arguments into the xml.  I built this as a Freemarker string template:

{xml}
<job>
    <name>PKSSendMultipleTrades</name>
    <desc>Multiple Random Trade Simulator.</desc>
    <request>
      <identifier>res:/pks/sendrandomtrade</identifier>
    </request>
    <simple>
      <startTime />
      <endTime />
      <repeatCount>${repeatCount}</repeatCount>
      <repeatInterval>${repeatInterval}</repeatInterval>
    </simple>
</job>
{/xml}

This configuration create a CRON job that issues a root request to '''res:/pks/sendrandomtrade''', starting immediately the job is created (via the empty <code><startTime /></code> element), and repeating <code>${repeatInterval}</code> times.

Then, the '''sendMultipleTrades''' endpoint, implemented as a DPML script, could call <code>active:freemarker</code> with the template and the arguments passed down from the calling request:

{xml}
<sequence>
	<request assignment="cronJobDefinition">
	    		<identifier>active:freemarker</identifier>
	    		<argument name="repeatCount">arg:repeatCount</argument>
	    		<argument name="repeatInterval">arg:repeatInterval</argument>
	    		<argument name="operator">res:/resources/endpoints/sendMultipleTrades.ftl</argument>
	</request>
	<request assignment="response">
	    <identifier>active:cronNew</identifier>
	    <argument name="job">this:cronJobDefinition</argument>
	</request>
</sequence>
{/xml}

Note that this DPML script uses a variable assignment to '''cronJobDefinition''', with the returned representation of the freemarker call (containing the <code><job></code> configuration document with the repeat arguments filled-in), which is passed into the '''active:cronNew''' CRON service request as the '''job''' argument.

Now the endpoint had been Constructed (without needing to write any physical code), we Compose the logical endpoint in the WWW space that is called from the '''simulator.html''' page:

{xml}
            <endpoint>
               <grammar>
               		<simple>res:/pks/sendmultipletrades/{interval}/{repeats}</simple>
               </grammar>
              <request>
                  <identifier>active:dpml</identifier>
                  <argument name="operator">res:/resources/endpoints/sendMultipleTrades.xml</argument>
                  <argument name="repeatCount" method="as-string">[[arg:repeats]]</argument>
	    		  <argument name="repeatInterval" method="as-string">[[arg:interval]]</argument>
               </request>
            </endpoint>
{/xml}

<blockquote>
It took me a little while to figure out how to get the '''repeats''' and '''interval''' argument values passed down into the CRON <code><job></code> xml resource, hence the perhaps odd use of freemarker string templating instead of XML - related technology (e.g. XRL2).  Something to look at again at some point...  
</blockquote>

Finally, it was now trivial to add some further UI components to the web simulator to trigger a HTTP GET request to resolve to the '''res:/pks/sendmultipletrades/{interval}/{repeats}''' URL.

Take a look at http://localhost:8080/pks/pub/simulator on your own system.

I could now trigger a sequence of periodic random simulated source transaction requests.  The NetKernel Visualiser showed the multiple repeated root requests with the '''res:/pks/sendrandomtrade''' identifier.

== Testing Transaction Throughput ==

=== Test Execution ===
The first performance target I tested was the Transaction Throughput requirement.  The goal was to achieve a sustained performance of approx 10ms per source transaction, with a received volume of 100 requests in a 10 second period (let's say an average of 100ms per request, ignoring finer - grained volume peaks of periods much less than 10 seconds).

Using the enhanced WWW simulator, I set up a run of 100 transactions, spaced out by an interval of 100ms, turned on the Visualiser, and clicked the '''Send multiple random''' button.  After an anxious wait of 10 seconds, I stopped the Visualiser.  

=== Test Outcome ===
The first good news was NK had taken the brunt of the load test, apparently with no ill effect.  The [[http://localhost:1060/tools/statuscharts | Status Graphs]] page showed a peak of approximateky 50 requests / second, and an increase in OldGen Heap usage of approx 50Mb (I suspect this was mostly the Visualiser traces being held in memory).  The CPU utilisation peaked briefly at 20% on my MacBookPro 2009. 

The second interesting thing to note was that the Visualiser table was now reporting it held traces of 104 root requests.      The shortest duration '''res:/pks/sendrandomtrade''' request was reported to be  23ms, the longest at 2485 ms (also the last) the second longest at 175ms.  

I could now inspect the complete Visualiser "time machine" trace information for a sample of these requests to quickly pinpoint where the time was being spent (it sure beats trawling through 10ms of Mbs of log4j application logs to extract end to end timing information!)

The 2 sub-requests of each root request that had the highest reported individual CPU usage (found easily by sorting the Visualiser request trace table by the '''CPU(ms)''' column) were '''active:pds''' SINK requests (5 - 10 ms each), followed by several HDSToXML TRANSREPT requests issued by the '''PDSH2Impl''' endpoint (each between 1.00 and 1.25 ms).  

The outlier request of 2485 ms reported a single XMLToHDS TRANSREPT request for an identifier '''pds:/pks/pos/EFX:LDN:PRE_STP:5039a5d7-0c5c-4ef5-a5f4-0276b55198dc:OPEN:2014-05-19:HKD''' with a CPU(ms) of 2461.87 ms (maybe there was a JVM GC collection at that point?)

=== Test Conclusion ===
<blockquote>
It is clear that our goal of '''10ms''' end-to-end has been exceeded between +100% to +500% in the current implementation.  However, it appears this is almost entirely (aside from a single as-yet unexplained outlier case) related to the NetKernel PDS persistence engine and the underlying H2 embedded database, at least as running on my MacBook (despite it having a [http://www.samsung.com/uk/consumer/memory-cards-hdd-odd/ssd/840-pro SSD drive] !). 
</blockquote>

Although not surprising, it highlights the old adage of avoiding direct SQL calls from a low-latency , high-throughput application if at all possible.  In the original, real-world platform in which the Java OO - based PKS operates,  it used a very fast write-only local persistence event source file (for restoring cache state on re-start) in line with the business logic.  An entirely separate Java application running on a different server was responsible for persisting trade and position message contents published on the middleware message bus into an archival SQL database. 

Also, Peter Rodgers pointed out to me that the PDSatabase) / embedded H2 combination is really only intended as a development tool, and certainly would not be recommended for use by NK-PKS in a production environment! 

Future work could include implementing a throughput throttle overlay, which would enforce a maximum rate of processed requests, or apply the Throttle Overlay to limit the number of concurrent trade requests being processed and limit the size of the waiting queue.

== Testing Capacity ==

Here, our target is simply to be able to persist up to 20,000 current (ie. with position date >= today) position records in the system.  This is a test of the capacity of the back-end persistence engine (in our case, the embedded H2 database) but also a test for the ability of Netkernel's representation cache to manage a larger number of records, with a combination of in-memory and off-line.

=== Capacity Simulator ===
First we are going to need to tweak the '''sendrandomtrade''' WWW simulator endpoint so that it will cause the settlement position builder to generate a far larger set of identifiable position records.  This was easily done by changing the single fixed position date with one from a 20,000 - long day range starting from today:

{java}
dateFormat = new SimpleDateFormat("yyyy-MM-dd")

...

  b.addNode("valueDate",generateRandomDateInRange(start, 20000, random))

...

// Generates a random date, by adding a random number of days between 0 and numDays to the start date
// Returns a string in format yyyy-mm-dd
def generateRandomDateInRange(start, numDays, random) {
	d = start.plus(random.nextInt(numDays))
	return dateFormat.format(d)
}
{/java}

=== Test Execution ===

Using the '''sendMultipleTrades''' WWW endpoint, I started a series of runs at different intervals, and observed the system behaviour during each.

run of 10 trades, spaced by a 1000ms interval.  System worked OK, and positions were generated (verified by using the ''' Display Positions''' button)

4 runs of 10 trades, spaced by 500ms, then 250ms, then 200ms then 100ms, all OK

A run of 100 trades, spaced by 100ms, all OK

A run of 1000 trades, spaced by 100ms, all OK

Then a repeat run of 1000 trades spaced by 100ms caused the system to queue up 10s of locked requests, as seen in the [[http://localhost:1060/tools/kernel | Requests and Threads]] control panel:

{literal}
3208	SOURCE res:/pks/sendrandomtrade as Object	223925	223925	LONG_WAIT	
3209	SOURCE res:/pks/sendrandomtrade as Object	223924	223923	LONG_WAIT	
3210	SOURCE res:/pks/sendrandomtrade as Object	223919	223919	LONG_WAIT	
...

WorkerThread0	WAITING	3211	SOURCE active:groovy+operator@res%3A/uk/co/rsbatechnology/pks/frontend/updatePositions.groovy+sourceTransaction@pbv%3Aoperand as Object
Lock name: [java.util.concurrent.CountDownLatch$Sync@5d1f2655] Lock owner: []
sun.misc.Unsafe.park() line:-2
java.util.concurrent.locks.LockSupport.park() line:186
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() line:834
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly() line:994
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly() line:1303
java.util.concurrent.CountDownLatch.await() line:236
org.netkernel.layer0.nkf.impl.NKFAsyncRequestHandleImpl.join() line:79
org.netkernel.layer0.nkf.INKFAsyncRequestHandle$join.call() line:-1
script1403457080694657427971$_run_closure2.doCall() line:41
sun.reflect.GeneratedMethodAccessor54.invoke() line:-1
WorkerThread1	WAITING	3215	SOURCE active:groovy+operator@res%3A/uk/co/rsbatechnology/pks/builders/settlementPositionBuilder.groovy+sourceTransaction@pbv%3Aoperand as Object
Lock name: [java.util.concurrent.CountDownLatch$Sync@7292361e] Lock owner: []
sun.misc.Unsafe.park() line:-2
java.util.concurrent.locks.LockSupport.park() line:186
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() line:834
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly() line:994
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly() line:1303
java.util.concurrent.CountDownLatch.await() line:236
org.netkernel.layer0.nkf.impl.NKFAsyncRequestHandleImpl.join() line:79
org.netkernel.layer0.nkf.INKFAsyncRequestHandle$join.call() line:-1
script14034570810331773121227.run() line:87
org.netkernel.lang.groovy.endpoint.GroovyRuntime.onRequest() line:37
{/literal}

It seems that the tasks are getting in some kind of deadlock situation, when there are a larger number of requests being queued up.  

I tried using NetKernel's Deadlock Detector, to at least stop the system freezing up.

{image}/doc/source/img:uk:co:rsbatechnology:pks:diary:part14:deadlockdetector{/image}

When I ran the test,I saw entries in the system console from the deadlock detector, showing it was killing LONG_WAIT Cron jobs, eg:

{literal}
 <ex>
  <id>RequestFrameError</id>
  <request>SOURCE active:groovy+operator@res%3A/uk/co/rsbatechnology/pks/frontend/updatePositions.groovy+sourceTransaction@pbv%3Aoperand as Object</request>
 </ex>
 <ex>
  <id>SubrequestError</id>
  <space>Lang / Groovy  (private)</space>
  <endpointId>GroovyRuntime</endpointId>
  <endpoint>org.netkernel.lang.groovy.endpoint.GroovyRuntime</endpoint>
  <ex>
   <id>NetKernelError</id>
  </ex>
  <ex>
   <id>deadlock detector kill</id>
  </ex>
 </ex>
</ex>
{/literal}

So, what was causing the deadlocks?  I had to go back and review the builder and posting code.  I suspect it was something to do with race conditions between async SINK persistence requests, and subsequent SOURCE requests.

{image}/doc/source/img:uk:co:rsbatechnology:pks:diary:part14:asyncrequests{/image}

Looking at the code closely, the '''settlementPositionBuilder''' endpoint asynchronously SINKs position deltas, and calls <code>join()</code> on the <code>INKFAsyncRequestHandle</code> handlers before sending its own response.  But the code does not set <code>context.setNoResponse()</code> before the Async requests, and therefore the kernel locks the worker thread, waiting for a response, [[doc:physicalreference:response | instead of releasing it]]. 

The '''updatePositions''' endpoint makes the same mistake.  I added a <code>context.setNoResponse</code> statement before the async requests:

{java}
	INKFRequest postingRequest = context.createRequest("active:positionPosting")
	postingRequest.addArgument("operand", deltaIdentifier)
	context.setNoResponse()
	asyncHandlers << context.issueAsyncRequest(postingRequest)
{/java}  

I then ran the 100ms * 100 trades test five times, looking at the Requests And Threads page, and saw no locked threads, and no deadlock detection.  The Status Graphs show a summary of the system performance (approx 20:51 to 20:54 in the screenshot below):

{image}/doc/source/img:uk:co:rsbatechnology:pks:diary:part14:statusgraphs{/image}

== Profile Overlay ==

I later came across the documentation of the [[doc:mod:architecture:profile | Profile Overlay]], which is part of the [[doc:mod:architecture:title | NKEE Enterprise Architectural Components]] module.  This is a simple way to add always-on monitoring statistics for requests following into a Space.

Following the docs, I wrapped the PKS Front End rootspace in the <code>ProfileOverlay</code> prototype:

{xml}
	<rootspace name="PKS Front-end Rootspace" uri="urn:uk:co:rsbatechnology:pks:frontend"
		public="true">
		<overlay>
			<prototype>mod.architecture.ProfileOverlay</prototype>
			<space>
				<mapper>
				...
								</mapper>
			</space>
		</overlay>
  		<!--  For ProfileOverlay -->
		<import>
			<uri>urn:com:ten60:netkernel:mod:architecture</uri>
		</import>
	</rootspace>
{/xml}

Running some test transactions through the PKS WWW simulator, I could then see some request statistics in the overlay's page in the explorer:

{image}/doc/source/img:uk:co:rsbatechnology:pks:diary:part14:profileoverlay{/image}

P
